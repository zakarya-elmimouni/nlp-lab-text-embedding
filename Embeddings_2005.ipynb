{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  NLP-lab :  Word embeddings\n",
    "\n",
    "In this series of exercises, we will explore three word embeddings:\n",
    "\n",
    "* [Collobert & Weston](http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf) https://ronan.collobert.com/senna/\n",
    "* [Word2Vec](https://arxiv.org/abs/1301.3781)\n",
    "* [BERT](https://huggingface.co/bert-base-uncased) \n",
    "\n",
    "\n",
    "In the code already provided, add your code to the place indicated by `YOUR CODE HERE`.\n",
    "\n",
    "**Important** : do NOT commit the data and embedding files in your repository git : it is a waste of resources and it takes more time to clone.\n",
    "> Use https://docs.github.com/en/get-started/getting-started-with-git/ignoring-files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "# display matplotlib graphics in notebook\n",
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "\n",
    "# disable warnings for libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# configure logger\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%I:%M:%S')\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Embeddings exploration with Collobert's embeddings\n",
    "\n",
    "Upload the files containing the embeddings to `data`:\n",
    "* Collobert (size 50): [collobert_embeddings.txt.zip](https://storage.teklia.com/shared/deepnlp-labs/collobert_embeddings.txt.zip) which contains the embedding vectors and [collobert_words.lst](https://storage.teklia.com/shared/deepnlp-labs/collobert_words.lst) which contains the associated words;\n",
    "\n",
    "You need to unzip the files to load them.\n",
    "\n",
    "Feel free to open the files to see what they contain (it's sometimes surprising).\n",
    "\n",
    "#### Question: \n",
    ">* Add the files to your .gitignore\n",
    ">* Give the size in Mb of the embeddings files before unzipping.\n",
    ">* By exploring the content of the embedding files, give the number of words for which these files provide embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:13:29 INFO:Number of words in Collobert embeddings: 130000\n",
      "09:13:29 INFO:Number of words in GloVe embeddings: 400000\n"
     ]
    }
   ],
   "source": [
    "# Path to the files\n",
    "collobert_words_path = 'collobert_embeddings.txt'\n",
    "glove_embeddings_path = 'glove.6B.50d.txt'\n",
    "\n",
    "# Read the Collobert words file\n",
    "with open(collobert_words_path, 'r') as file:\n",
    "    collobert_words = file.readlines()\n",
    "\n",
    "# Read the GloVe embeddings file\n",
    "with open(glove_embeddings_path, 'r',encoding='utf-8') as file:\n",
    "    glove_embeddings = file.readlines()\n",
    "\n",
    "# Number of words in each file\n",
    "num_collobert_words = len(collobert_words)\n",
    "num_glove_words = len(glove_embeddings)\n",
    "\n",
    "# Log the results\n",
    "logger.info(f'Number of words in Collobert embeddings: {num_collobert_words}')\n",
    "logger.info(f'Number of words in GloVe embeddings: {num_glove_words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of closest words\n",
    "\n",
    "The aim of this exercise is to list the closest words to a given word for the Collobert embedding. First, we'll load the vectors of the Collobert embedding into a numpy array and the associated words into a python list. Then we'll use the [scipy KDTree](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.html) data structure to quickly search for the vectors closest to a series of words.\n",
    "\n",
    "\n",
    "#### Question: \n",
    ">* load embedding vectors from the file `data/collobert_embeddings.txt` using the numpy function [genfromtxt](https://numpy.org/doc/stable/reference/generated/numpy.genfromtxt.html)\n",
    ">* load the words associated with the vectors from the `data/collobert_words.lst` file into a python list (using `open()` and `readlines()`)\n",
    ">* check that the sizes are correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings: 130000\n",
      "Number of words: 130000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the embeddings\n",
    "collobert_embeddings = np.genfromtxt('collobert_embeddings.txt')\n",
    "\n",
    "# Load the words\n",
    "with open('collobert_words.lst', 'r',encoding='utf-8') as file:\n",
    "    collobert_words = file.readlines()\n",
    "\n",
    "# Verify the sizes\n",
    "assert collobert_embeddings.shape[0] == len(collobert_words), \"Mismatch in number of embeddings and words\"\n",
    "\n",
    "print(f\"Number of embeddings: {collobert_embeddings.shape[0]}\")\n",
    "print(f\"Number of words: {len(collobert_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KD trees are a very efficient data structure for storing large sets of points in a multi-dimensional space and performing very efficient nearest-neighbour searches. \n",
    "\n",
    "#### Question \n",
    "> * Initialise the [KDTree](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.html) structure with Collobert's embedding vectors.\n",
    "> * Using the [tree.query](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.query.html#scipy.spatial.KDTree.query) function, display the 5 nearest words for the following words: ‘mother’, ‘computer’, ‘dentist’, ‘war’, ‘president’, ‘secretary’, ‘nurse’.  *Hint: you can use the function `collobert_words.index(w)` to obtain the index of a word in the list of words*.\n",
    "> * Create a `words_plus_neighbors` list containing the words and all their neighbours (for the next question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: mother, Neighbors: ['mother\\n', 'daughter\\n', 'wife\\n', 'father\\n', 'husband\\n']\n",
      "Word: computer, Neighbors: ['computer\\n', 'laptop\\n', 'multimedia\\n', 'desktop\\n', 'software\\n']\n",
      "Word: dentist, Neighbors: ['dentist\\n', 'pharmacist\\n', 'midwife\\n', 'physician\\n', 'housekeeper\\n']\n",
      "Word: war, Neighbors: ['war\\n', 'revolution\\n', 'death\\n', 'court\\n', 'independence\\n']\n",
      "Word: president, Neighbors: ['president\\n', 'governor\\n', 'chairman\\n', 'mayor\\n', 'secretary\\n']\n",
      "Word: secretary, Neighbors: ['secretary\\n', 'minister\\n', 'treasurer\\n', 'chairman\\n', 'commissioner\\n']\n",
      "Word: nurse, Neighbors: ['nurse\\n', 'physician\\n', 'veterinarian\\n', 'dentist\\n', 'surgeon\\n']\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import KDTree\n",
    "\n",
    "# Initialize KDTree with Collobert embeddings\n",
    "tree = KDTree(collobert_embeddings)\n",
    "\n",
    "# List of words to find neighbors for\n",
    "words_to_query = ['mother', 'computer', 'dentist', 'war', 'president', 'secretary', 'nurse']\n",
    "\n",
    "# Find the 5 nearest neighbors for each word\n",
    "words_plus_neighbors = []\n",
    "for word in words_to_query:\n",
    "    word_index = collobert_words.index(word+'\\n' ) \n",
    "    distances, indices = tree.query(collobert_embeddings[word_index], k=5)\n",
    "    neighbors = [collobert_words[i] for i in indices]\n",
    "    print(f\"Word: {word}, Neighbors: {neighbors}\")\n",
    "    words_plus_neighbors.extend(neighbors)\n",
    "\n",
    "# Remove duplicates\n",
    "words_plus_neighbors = list(set(words_plus_neighbors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation with T-SNE\n",
    "\n",
    "Embeddings are vectors with several hundred dimensions. It is therefore not possible to display them in their original space. However, it is possible to apply dimension reduction algorithms to display them in 2 or 3 dimensions. One of the dimension reduction algorithms allowing 2D visualisation is [tSNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding). \n",
    "\n",
    "#### Question\n",
    "> * Create a `word_vectors` object of type `np.array` from a list containing all the embeddings of the words in the `words_plus_neighbors` list.\n",
    "> * Create a tSNE object from the `from sklearn.manifold import TSNE` library with the parameters `random_state=0`, `n_iter=2000` and `perplexity=15.0` for a 2-dimensional view.\n",
    "> * Calculate *T* the tSNE transformation of the `word_vectors` by applying function `.fit_transform(word_vectors)` to the tSNE object. This function estimates the parameters of the tSNE transformation and returns the reduced-dimension representation of the vectors used for estimation.\n",
    "> * Use the `scatterplot` function from [seaborn](https://seaborn.pydata.org/generated/seaborn.scatterplot.html) to represent points in 2 dimensions and add word labels using the `plt.annotate` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Create an object `word_vectors` of type `np.array` from a list containing all the embeddings of the words in the list `words_plus_neighbors`\n",
    "\n",
    "word_vectors = np.array([collobert_embeddings[collobert_words.index(word)] for word in words_plus_neighbors])\n",
    "\n",
    "# Create a tSNE object from the sklearn.manifold library with the parameters random_state=0, n_iter=2000, and perplexity=15.0 for a 2D visualization\n",
    "tsne = TSNE(random_state=0, n_iter=2000, perplexity=15.0)\n",
    "\n",
    "# Compute the tSNE transformation of the word_vectors by applying the .fit_transform(word_vectors) function to the tSNE object\n",
    "T = tsne.fit_transform(word_vectors)\n",
    "\n",
    "# Plot the results\n",
    "fig = plt.figure()\n",
    "fig.patch.set_facecolor('#f9f9f9')\n",
    "\n",
    "sns.set(rc={'figure.figsize':(14, 8)})\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "sns.scatterplot(x=T[:, 0], y=T[:, 1])\n",
    "\n",
    "for label, x, y in zip(words_plus_neighbors, T[:, 0], T[:, 1]):\n",
    "    plt.annotate(label.strip(), xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic arithmetic with Word2Vec\n",
    "\n",
    "One of the most original properties of Word2Vec embeddings is that the semantic relationships between vectors can be modelled by arithmetic operations. Given vectors representing the words `king`, `man` and `woman`, it is possible to compute the vector `v` as :  \n",
    "\n",
    "`v = vector(king)-vector(man)+vector(woman)`\n",
    "\n",
    "This operation corresponds to the following semantic relationship: *The king is to the man what the queen is to the woman*, which translates into the following arithmetic: *the concept of king, minus the concept of man plus the concept of woman gives the concept of queen*.\n",
    "\n",
    "In fact, if we look in the embedding for the word whose closest vector is `v`, we find `reine`.\n",
    "\n",
    "\n",
    "We will use a Word2Vec model pre-trained on the French Wac corpus.  This model has been trained on a corpus of 1 billion French words. \n",
    "\n",
    "This embedding is available in 2 formats:\n",
    "- a text format for easy exploration of the model :\n",
    "    - frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.txt](https://storage.teklia.com/shared/deepnlp-labs/frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.txt)\n",
    "- a binary format that can be loaded using the Gensim library: \n",
    "    - [enWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin](https://storage.teklia.com/shared/deepnlp-labs/frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin)\n",
    "\n",
    "Download the text file onto your machine to analyse it.\n",
    "\n",
    "#### Question: \n",
    ">* Add the file to your .gitignore\n",
    ">* Give the size in Mb of the embedding files\n",
    ">* By exploring the contents of the embedding file in text format, give the number of words for which this model provides embeddings and the size of the embedding for each word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Collobert embedding file: 120.21 Mb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "fichierbin=\"frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin\"\n",
    "# Get the size of the embedding file in Mb\n",
    "ficiher_bin_size_mb = os.path.getsize(fichierbin) / (1024 * 1024)\n",
    "\n",
    "\n",
    "print(f\"Size of Collobert embedding file: {ficiher_bin_size_mb:.2f} Mb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word similarity\n",
    "\n",
    "We are now going to use the [Gensim] library (https://radimrehurek.com/gensim/) to load the Word2Vec model and use it. \n",
    "\n",
    "#### Question: \n",
    ">* Modify the following code to load the Word2Vec template file in binary format using [load_word2vec](https://radimrehurek.com/gensim/models/keyedvectors.html#how-to-obtain-word-vectors)\n",
    ">* Choose a couple of words and find the closest words according to the model using [most_similar](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar)\n",
    ">* To guess the meaning of the words ‘yokohama’, ‘kanto’ and ‘shamisen’, look for their nearest neighbours. Explain the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:21:31 INFO:loading projection weights from frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin\n",
      "09:21:31 INFO:KeyedVectors lifecycle event {'msg': 'loaded (155562, 200) matrix of type float32 from frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2025-02-15T21:21:31.764679', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:27:36) [GCC 11.2.0]', 'platform': 'Linux-5.14.0-503.23.2.el9_5.x86_64-x86_64-with-glibc2.34', 'event': 'load_word2vec_format'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('commandeur', 0.6844523549079895),\n",
       " ('chevaliers', 0.6799763441085815),\n",
       " ('écuyer', 0.6333731412887573),\n",
       " ('grand-croix', 0.621898353099823),\n",
       " ('preux', 0.6011075377464294),\n",
       " ('chevalerie', 0.5404021143913269),\n",
       " ('légion', 0.5335969924926758),\n",
       " ('honneur', 0.4953608810901642),\n",
       " ('yvain', 0.4855087101459503),\n",
       " ('insignes', 0.4742659330368042)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "## YOUR CODE HERE\n",
    "\n",
    "embedding_file =\"frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin\"\n",
    "model = KeyedVectors.load_word2vec_format(embedding_file, binary=True, unicode_errors=\"ignore\")\n",
    "## YOUR CODE HERE\n",
    "model.most_similar(\"chevalier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tokyo', 0.7117858529090881),\n",
       " ('tôkyô', 0.6314416527748108),\n",
       " ('japon', 0.6215220093727112),\n",
       " ('nagoya', 0.61984783411026),\n",
       " ('kyushu', 0.6141085028648376),\n",
       " ('osaka', 0.6123895049095154),\n",
       " ('fukuoka', 0.5612888336181641),\n",
       " ('japonaise', 0.5507327318191528),\n",
       " ('sendai', 0.5496150255203247),\n",
       " ('japonais', 0.5391373038291931)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"yokohama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the word is coming from japanese , so the nearest words to it are related to japan, and have not necessarily any common semantic with \"yokohama\". it's logic because the model is trained on frensh words not on japanese ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.most_similar(\"shamisen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the word is unkonwn for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pokémon', 0.5426284670829773),\n",
       " ('mewtwo', 0.5076008439064026),\n",
       " ('pokémons', 0.4970633387565613),\n",
       " ('saito', 0.4549728333950043),\n",
       " ('pokédex', 0.448673278093338),\n",
       " ('yusuke', 0.4416310787200928),\n",
       " ('osaka', 0.4372846484184265),\n",
       " ('shôgun', 0.4324426054954529),\n",
       " ('jin', 0.42604967951774597),\n",
       " ('honshu', 0.42374101281166077)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"kanto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the same conclusion as before holds here: the model is trained on frensh words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semantic arithmetic\n",
    "\n",
    "One of the most original properties of Word2Vec embeddings is that the semantic relationships between vectors can be modelled by arithmetic operations. Given vectors representing the words `king`, `man` and `woman`, it is possible to compute the vector `v` as :  \n",
    "\n",
    "`v = vector(king)-vector(man)+vector(woman)`\n",
    "\n",
    "This operation corresponds to the following semantic relationship: *The king is to the man what the queen is to the woman*, which translates into the following arithmetic: *the concept of king, minus the concept of man plus the concept of woman gives the concept of queen*.\n",
    "\n",
    "In fact, if we look in the embedding for the word whose closest vector is `v`, we find `reine`.\n",
    "\n",
    "#### Question: \n",
    ">* using the function [most_similar](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar) specifying the arguments `positive` for the vectors to be added and `negative` for the vectors to be subtracted, check the relationship *the concept of king, minus the concept of man plus the concept of woman gives the concept of queen*.\n",
    ">* Using the same method, find XXX in the following semantic relations\n",
    ">   * Paris is to France what XXX is to Japan.\n",
    ">   * Chevalier is to France what XXX is to Japan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king - man + woman = jessica\n",
      "Paris is to France what tokyo is to Japan\n",
      "Chevalier is to France what écuyer is to Japan\n"
     ]
    }
   ],
   "source": [
    "# Check the relationship: king - man + woman = queen\n",
    "result_king = model.most_similar(positive=['king', 'woman'], negative=['man'])\n",
    "print(f\"king - man + woman = {result_king[0][0]}\")\n",
    "\n",
    "#capital of France is Paris\n",
    "result_paris = model.most_similar(positive=['paris', 'japan'], negative=['france'])\n",
    "print(f\"Paris is to France what {result_paris[0][0]} is to Japan\")\n",
    "\n",
    "#  Chevalier is to France what .... is to Japan\n",
    "result_chevalier = model.most_similar(positive=['chevalier', 'japan'], negative=['france'])\n",
    "print(f\"Chevalier is to France what {result_chevalier[0][0]} is to Japan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual embeddings with BERT \n",
    "\n",
    "BERT was one of the first freely available Transformer language models, trained on large corpora. Many other models are available on HuggingFace.\n",
    "\n",
    "As BERT is a contextual model, it is necessary to have it predict whole sentences in order to study the word embeddings it produces. In this section, we will compare the embeddings obtained for polysemous words according to the sentence in which they are used.\n",
    "\n",
    "In English, *plant* has two meanings: plant and vegetable. With a non-contextual embedding, such as Glove or Colobert, these two meanings of the word plus are associated with an identical embedding. With BERT, we'll see that the same word can have several embeddings depending on the context.\n",
    "\n",
    "First, load the BERT model and tokenizer from HuggingFace : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ba0fd2ab514cecba2ede5599554949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6903edc038884f6990ef01af77ee8476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa85ad5579474da3b215af2b831aa140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fba9899f64c4641a249f9362a683657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c58702764ea4f80ad1e7a32480cfefa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# Load pre-trained model \n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # to access the hidden states\n",
    "                                  )\n",
    "# set the model to \"evaluation\" mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "Language models are trained with a specific breakdown of sentences into tokens. These tokens can be words or parts of words. It is necessary to use the tokenizer corresponding to each model.\n",
    "\n",
    "tokenizer.vocab.keys() gives the list of all the tokens known for the language model. \n",
    "\n",
    "#### Question\n",
    ">* How many different tokens are known to the BERT tokenizer?\n",
    ">* Display a hundred tokens at random. What do you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the tokenizer: 30522\n",
      "Sample of 100 tokens: ['##ᄆ', 'transvaal', '##ua', 'treacherous', '##ʔ', 'noodles', 'greyish', '##ης', 'quo', 'stanley', '##cky', 'rebuilding', 'holds', 'kris', 'pushing', 'collaborating', 'reckless', '##bor', 'spreads', '≡', 'mosque', '##dner', 'waltz', 'friendly', 'horribly', 'judaism', 'engine', 'formed', 'walkover', 'gasp', 'antoinette', 'malls', 'cher', 'poem', 'saxophonist', 'bus', '##yin', 'landing', '##rid', '[unused431]', 'soaked', 'knelt', 'alarmed', 'audible', 'unnamed', 'plot', 'rowan', 'dissipated', 'reprint', 'winged', 'semitic', '⁻', 'pitted', 'morale', 'figuring', 'entities', '288', 'rip', 'disbelief', '##ier', 'kenyan', 'roberts', 'localized', '##lla', 'igor', 'outspoken', 'jewelry', 'liberties', '##shin', 'blankets', 'evolved', 'taxonomic', 'mitochondrial', '[unused902]', 'colliery', 'associate', '##relli', 'pantry', '##bi', 'background', '##pipe', 'caracas', 'craft', 'trier', 'doubles', '##olt', 'inverness', '##ת', 'stroll', 'pinched', 'varied', 'bradford', 'clan', 'circa', 'suspended', 'cain', 'kapoor', 'disagree', '##rock', 'leonard']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# number of token in tokenizer\n",
    "num_tokens = len(tokenizer)\n",
    "print(f\"Number of tokens in the tokenizer: {num_tokens}\")\n",
    "\n",
    "# sample of 100 tokens\n",
    "sample_tokens = random.sample(list(tokenizer.vocab.keys()), 100)    \n",
    "print(f\"Sample of 100 tokens: {sample_tokens}\")\n",
    "# YOU CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Answer:\n",
    "we can see that there is some tokens not making sens like ##ᄆ, or some end of words like ##rid  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer splits sentences and transforms the elements (words or sub-words) into clues. \n",
    "\n",
    "BERT can process several sentences, but you need to tell it how the sentences (segments) have been split, with an index: 0 for the first sentence, 1 for the second. \n",
    "\n",
    "Two specific tokens must also be added: \n",
    "* CLS], a specific token used for sentence classification\n",
    "* SEP], the end of sentence token.\n",
    "\n",
    "#### Question\n",
    ">* Apply the bert_tokenize function to the 3 phases and keep the 3 vectors (index, token, segment).\n",
    ">* Display this information for each of the sentences and check that the word *plant* has the same token index in the two sentences in which it appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: ([101, 1996, 3269, 2038, 2584, 2049, 29160, 2504, 1997, 2537, 1012, 102], ['[CLS]', 'the', 'plant', 'has', 'reached', 'its', 'maximal', 'level', 'of', 'production', '.', '[SEP]'], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Sentence 2: ([101, 1996, 3765, 2024, 9240, 2503, 1996, 4713, 1012, 102], ['[CLS]', 'the', 'cars', 'are', 'assembled', 'inside', 'the', 'factory', '.', '[SEP]'], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Sentence 3: ([101, 1037, 3269, 3791, 9325, 1998, 2300, 2000, 4982, 2092, 1012, 102], ['[CLS]', 'a', 'plant', 'needs', 'sunlight', 'and', 'water', 'to', 'grow', 'well', '.', '[SEP]'], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "snt1 = \"The plant has reached its maximal level of production.\"\n",
    "snt2 = \"The cars are assembled inside the factory.\"\n",
    "snt3 = \"A plant needs sunlight and water to grow well.\"\n",
    "\n",
    "\n",
    "def bert_tokenize(snt):\n",
    "    \"\"\" Apply the BERT tokenizer to a list of words representing a sentence\n",
    "        and return 3 lists: \n",
    "        - list of token indx\n",
    "        - list of token for debugging, not used by the BERT model\n",
    "        - list of sentence index\n",
    "        \"\"\"\n",
    "    # Add the special tokens.\n",
    "    tagged_snt = \"[CLS] \" + snt + \" [SEP]\" \n",
    "    # Tokenize\n",
    "    tokenized_snt = tokenizer.tokenize(tagged_snt)\n",
    "    # convert tokens to indices\n",
    "    indexed_snt = tokenizer.convert_tokens_to_ids(tokenized_snt)\n",
    "    # mark the words in sentence.\n",
    "    segments_ids = [1] * len(tokenized_snt)\n",
    "\n",
    "    return (indexed_snt, tokenized_snt, segments_ids)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "indexed_snt1, tokenized_snt1, segments_ids1=bert_tokenize(snt1)\n",
    "indexed_snt2, tokenized_snt2, segments_ids2=bert_tokenize(snt2)\n",
    "indexed_snt3, tokenized_snt3, segments_ids3=bert_tokenize(snt3)\n",
    "\n",
    "print(f\"Sentence 1: {indexed_snt1, tokenized_snt1, segments_ids1}\")\n",
    "print(f\"Sentence 2: {indexed_snt2, tokenized_snt2, segments_ids2}\")\n",
    "print(f\"Sentence 3: {indexed_snt3, tokenized_snt3, segments_ids3}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " the word *plant* has the same token index in the two sentences in which it appears."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "To calculate embeddings, we need to make a prediction using the BERT model on a complete sentence. The *predict_hidden* function converts the token and segment index lists into a pytorch tensor and applies the model. \n",
    "\n",
    "The model used is a 12-layer model. We will use the last hidden layer of the model as an embedding to represent the words. Other solutions are possible, such as concatenation or averaging of several layers.\n",
    "\n",
    "\n",
    "#### Question\n",
    ">* Apply the model to each of the 3 sentences and store the resulting embeddings (tensors).\n",
    ">* Display the dimension of the resulting tensors. What is the dimension of the embedding vector for each word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions of hidden_states_snt1 torch.Size([12, 768])\n",
      "dimensions of hidden_states_snt2 torch.Size([10, 768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def predict_hidden(indexed_snt, segments_ids):\n",
    "    \"\"\"Apply the BERT model to the input token indices and segment indices\n",
    "        and return the last hidden layer\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Convert inputs to PyTorch tensors\n",
    "        tokens_tensor = torch.tensor([indexed_snt])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        hidden_states = outputs[2]\n",
    "        one_hidden_layer = hidden_states[12][0]\n",
    "        \n",
    "    return one_hidden_layer\n",
    "\n",
    "# YOUR CODE HERE\n",
    "hidden_states_snt1 = predict_hidden(indexed_snt1, segments_ids1)\n",
    "hidden_states_snt2 = predict_hidden(indexed_snt2, segments_ids2)\n",
    "hidden_states_snt3 = predict_hidden(indexed_snt3, segments_ids3)\n",
    "print(\"dimensions of hidden_states_snt1\",hidden_states_snt1.shape)\n",
    "print(\"dimensions of hidden_states_snt2\",hidden_states_snt2.shape)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer: the dimension of the embedding vector for each word in the sentence is 768\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden layer returned by the *predict_hidden* function is a tensor containing a context vector representing each token in the input sentence. We can use this vector to represent the meaning of this word as a function of its context. We're going to compare the representation of the polysemous word *plant* as a function of its context.\n",
    "\n",
    "#### Question\n",
    ">* Using the [cosine distance](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html), calculate the following distances:\n",
    "> * distance between *plant* in sentence 1 (plant-factory) and *plant* in sentence 3 (plant-vegetal)\n",
    "> * distance between *plant* in sentence 1 (plant-factory) and *factory* in sentence 2 (plant-vegetal) \n",
    "> * distance between *plant* in sentence 1 (plant-factory) and *production* in sentence 2 \n",
    "> * distance between *plant* in sentence 3 (plant-vegetal) and *production* in sentence 2 \n",
    "> How can we interpret these distances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The plant has reached its maximal level of production.\n",
      "The cars are assembled inside the factory.\n",
      "A plant needs sunlight and water to grow well.\n"
     ]
    }
   ],
   "source": [
    "print(snt1)\n",
    "print(snt2)\n",
    "print(snt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distance between *plant* in sentence 1 and *plant* in sentence 3: 0.50\n",
      "Cosine distance between *plant* in sentence 1 and *factory* in sentence 2: 0.69\n",
      "Cosine distance between *production* in sentence 1 and *plant* in sentence 3: 0.38\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# distance between *plant* in sentence 1 (plant-factory) and *plant* in sentence 3 (plant-vegetal)\n",
    "cosine_distance1 = 1 - cosine(hidden_states_snt1[2], hidden_states_snt3[2])\n",
    "print(f\"Cosine distance between *plant* in sentence 1 and *plant* in sentence 3: {cosine_distance1:.2f}\")\n",
    "\n",
    "\n",
    "# distance between *plant* in sentence 1 (plant-factory) and *factory* in sentence 2 (plant-factory)\n",
    "cosine_distance2 = 1 - cosine(hidden_states_snt1[2], hidden_states_snt2[7])\n",
    "print(f\"Cosine distance between *plant* in sentence 1 and *factory* in sentence 2: {cosine_distance2:.2f}\")\n",
    "\n",
    "# distance between *production* in sentence 1 (plant-factory) and *plant* in sentence 3 (plant-vegetal)\n",
    "cosine_distance3 = 1 - cosine(hidden_states_snt1[9], hidden_states_snt3[2])\n",
    "print(f\"Cosine distance between *production* in sentence 1 and *plant* in sentence 3: {cosine_distance3:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that the cosine distance between the word plant in sentence 1 and the word plant in sentence 3 is 0.5 which is not high, this means that the two words are not similar semantically, despiste the fact that they are the same spelling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
